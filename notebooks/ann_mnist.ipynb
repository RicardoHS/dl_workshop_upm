{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "This notebook introduces some aspects of the Keras API, demonstrated on the MNIST handwritten digit data set (a classic benchmark in computer vision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the MNIST data set (integrated in Keras)\n",
    "from keras.datasets import mnist\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a random instance\n",
    "i = np.random.choice(np.arange(x_train_mnist.shape[0]), 1)[0]\n",
    "print '({}) Label: {}'.format(i, y_train_mnist[i])\n",
    "plt.imshow(x_train_mnist[i],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The input data consists of 28x28 images. However, models like logistic regression or regular neural networks\n",
    "# do not understand 2D input. Therefore, we flatten the input images to convert them into 1D vectors\n",
    "x_train_flat = x_train_mnist.reshape([60000, 28*28])\n",
    "x_test_flat = x_test_mnist.reshape([10000, 28*28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a logistic (softmax) regression model on MNIST to demonstrate poor performance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(x_train_flat,y_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print accuracy to evaluate the performance of logistic regression\n",
    "from sklearn import metrics\n",
    "print 'Train accuracy: {}'.format(metrics.accuracy_score(y_train_mnist, clf.predict(x_train_flat)))\n",
    "print 'Test accuracy: {}'.format(metrics.accuracy_score(y_test_mnist, clf.predict(x_test_flat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a neural network. Option 1: 1 layer (equivalent to logistic regression)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, input_dim=28*28))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print a representation of the network architecture\n",
    "# It's always a good idea to print this visualization of the network, to make sure we have built what we had in mind\n",
    "# Also, it is important to know how many parameters we are going to have to estimate.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, input_dim=28*28))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_splits(X, y, ratio=0.1, cat=False):\n",
    "    \"\"\"\n",
    "    Finds a random split of size ratio*size(data).\n",
    "    Returns the corresponding splits of X and y.\n",
    "    \"\"\"\n",
    "    val_ids = np.random.choice(np.arange(X.shape[0]), int(X.shape[0]*ratio), replace=False)\n",
    "    train_ids = np.delete(np.arange(X.shape[0]), val_ids)\n",
    "    x_train = X[train_ids,:]\n",
    "    x_val = X[val_ids,:]\n",
    "    if cat:\n",
    "        y_train = y[train_ids,:]\n",
    "        y_val = y[val_ids,:]\n",
    "    else:\n",
    "        y_train = y[train_ids]\n",
    "        y_val = y[val_ids]                \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a very important step when training neural networks.\n",
    "# Since the objective function is usually very complex, the optimization algorithm can take many iterations.\n",
    "# In addition, if the network is sufficiently complex it can overfit the training set.\n",
    "#\n",
    "# This problem can be diminished by keeping aside a small split of the training data and periodically evaluating\n",
    "# the loss of the model on it. \n",
    "x_train, y_train, x_val, y_val = get_splits(x_train_flat, y_train_mnist, ratio=0.05, cat=False)\n",
    "print 'Train: {}. Validation: {}'.format(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To optimize objectives like categorical cross entropy (adequate for softmax output), we need to convert\n",
    "# the labels to one-hot encoding (e.g. label 2 turns into [0,0,1,0,0,0,0,0,0,0])\n",
    "from keras import utils\n",
    "y_train = utils.to_categorical(y_train, num_classes=10)\n",
    "y_val = utils.to_categorical(y_val, num_classes=10)\n",
    "y_test = utils.to_categorical(y_test_mnist, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example of one-hot encoded label\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the model for 10 epochs with full gradient\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=1024, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists for monitoring progress\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Minibatch gradient descent\n",
    "# Here we try another approach. Usually, neural networks are not trained computing the full gradient,\n",
    "# but only the gradient on a small batch of data (usually a not too large power of 2).\n",
    "# This can help avoid overfitting, escape local minima and get more frequent progress reports\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "for i in range(50):\n",
    "    history = model.fit(x_train, y_train, epochs=1, batch_size=128, verbose=1, validation_data=(x_val, y_val))\n",
    "    \n",
    "    train_loss.append(history.history['loss'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    \n",
    "    ax.clear()    \n",
    "    ax.plot(train_loss, color='red', label='Train')\n",
    "    ax.plot(val_loss, color='blue', label='Validation')\n",
    "\n",
    "    fig.canvas.draw()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember you can get the weights as an array, if you a re curious and want to take a look\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also easily save the model and its weights for later use\n",
    "model.save('ann_mnist_64_relu.h5')\n",
    "model.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def eval(y, preds, classes=None):\n",
    "    if not classes:\n",
    "        classes = np.unique(y)\n",
    "    \"\"\"\n",
    "    Given a set of labels y and predictions preds, computes precision, recall and F1.\n",
    "    \"\"\"\n",
    "    for i in classes:\n",
    "        preds_i = [1 if j==i else 0 for j in preds]\n",
    "        y_i = [1 if j==i else 0 for j in y]\n",
    "        print 'Class {}:'.format(i)    \n",
    "        print 'Precision: {}'.format(metrics.precision_score(y_i, preds_i))\n",
    "        print 'Recall: {}'.format(metrics.recall_score(y_i, preds_i))\n",
    "        print 'F1: {}'.format(metrics.f1_score(y_i, preds_i))\n",
    "        print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "# First, we tell the model to give predictions for the test set\n",
    "preds = model.predict(x_test_flat)\n",
    "# Then, we convert the softmax predictions to label form\n",
    "preds = map(lambda x: np.argmax(x), preds)\n",
    "# Now, print some classification metrics (the function above)\n",
    "eval(y_test_mnist, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Guess a random number from the test set\n",
    "x = np.random.choice(np.arange(x_test_flat.shape[0]), 1)\n",
    "x = x_test_flat[x]\n",
    "img_x = x.reshape([28,28])\n",
    "print np.argmax(model.predict(x))\n",
    "plt.imshow(img_x,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Guess the number we just wrote by hand!\n",
    "# Try it at home: draw a number on a 28x28 black background, using any Paint-like app. See if the model can guess it\n",
    "img=mpimg.imread('number.png')\n",
    "x = img[:,:,0].reshape([1,28*28])\n",
    "print np.argmax(model.predict(x))\n",
    "plt.imshow(img,cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
